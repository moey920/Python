## 코로나 날짜별 확진자 크롤링하기

### 모듈화에 집중하여 이용해보기.
```
import requests

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.368'}
url = "http://openapi.seoul.go.kr:8088/4349786750726d6139356b486b7272/json/Corona19Status/1/1000/"
response = requests.get(url, headers=headers)
data = response.json()

def parse_result(**kwargs) : #**kwargs의 장점은 가독성이다.
    # 날짜별 확진자(날짜가 키 값)
    # 코드를 어떻게 짜야할지 생각해보고 코딩하기
    # 1. 키 값이 존재하는가? 키 값이 없다면 어떻게 처리할 것인가? -> 최초일 경우 1을 추가
    # 2. 키 값의 value가 1으로 이미 존재한다면, value를 1 증가 시킨다.
    # 3. 생각해논 해답을 파이썬 문법으로 옮기기.

    data_dict = {} 
    rows = data['Corona19Status']['row']
    for row in rows : 
        date = row[kwargs['target']] # 키 값(날짜)
    # data_dict[date] = 1
    # print(data_dict)
        if date in data_dict : # 2012-01-12에 데이터가 들어있는가?
            data_dict[date] += 1
        else :
            data_dict[date] = 1

    print(data_dict)


    # 지역별 확진자

    rows = data['Corona19Status']['row']
    for row in rows : 
        date = row[kwargs['target']] # 키 값(날짜)
    # data_dict[date] = 1
    # print(data_dict)
        if date in data_dict : # 2012-01-12에 데이터가 들어있는가?
            data_dict[date] += 1
        else :
            data_dict[date] = 1

    print(data_dict)
    
target_list = ["CORONA19_DATE", "CORONA19_AREA", "CORONA19_CONTACT_HISTORY"]
parse_result(target="CORONA19_DATE")

for target in target_list :
    parse_result(target=target)

```

## 강남구의 면적당 공시지가 크롤링하기

## 네이버 증권 크롤링하기
```
# 네이버 증권에서 원하는 항목의 주가를 파싱해봅시다.
import time
from random import randint # 서버에 주기적으로 요청하면, ip를 막아버리는 경우가 생겨서 random 모듈을 이용한다.
import requests
from bs4 import BeautifulSoup


def parse_stock(company_code) :

    headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.368'}
    url = "https://finance.naver.com/item/main.nhn?code=" + company_code  # SAMSUNG
    
    response = requests.get(url, headers=headers)
    if response.status_code != 200 :
        return print("Unable to access")
    soup = BeautifulSoup(response.text, 'html.parser')

    # print(soup.title.string)
    # print(soup.title.parent)
    # print(soup.find_all(id='u_skip'))
    # print(soup.find_all(class_='no_today'))

    # 개발자도구에 selector를 지원함.
    # chart_area > div.rate_info > div > p.no_today > em
    # result = soup.select("div > p.no_today > em > span.blind")
    stock = soup.select_one("div > p.no_today > em > span.blind").text

    name = soup.select_one("#middle > div.h_company > div.wrap_company > h2 > a").text
    print(name, stock)

company_list = ['005380', '005930', '066570']

start = time.time() # 현재 시간 측정
PERIOD_OF_TIME = 10 # 진행 시간 설정

while True :
    # 랜덤 주기로 서버에 요청하기 위한 구문
    time.sleep(randint(1, 4)) # 1~4초 동안 무작위로 쉰다.
    for company in company_list :
        parse_stock(company)

    if time.time() > start + PERIOD_OF_TIME :
        break

print("Done!")
    
# 주식 종목코드 확인 방법!
# https://kind.krx.co.kr/corpgeneral/corpList.do?method=loadInitPage
```
